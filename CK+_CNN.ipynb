{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d689871-5770-4d2e-b3b6-801019cead8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend \n",
    "import numpy as np\n",
    "import preprocess\n",
    "import firstCNN\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import  layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cd155b-b79d-4918-b14e-6604b1971675",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/Users/kristshingjergji/Desktop/images_datasets/CK+/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5165cd-5097-4cf5-8017-964f323c4f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = os.listdir(folder + 'cohn-kanade-images/')\n",
    "participants.sort()\n",
    "print(f'Choose a subject among the following {(len(participants)-1)} subjects:') \n",
    "\n",
    "print(participants)\n",
    "person = input()\n",
    "print(f'Choose session for subject {person}') \n",
    "session = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1464e5d3-0fb4-4633-a88a-c3a6cd64b7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "AU, landmarks, image, imageLand = preprocess.landmarks_AU_CK(person, session, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f4334-1a43-403b-98ea-d1673824dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing \n",
    "# 1. Align using landmarks of eyes\n",
    "image_aligned, landmarks_aligned = preprocess.alignFromLandmarks(image, landmarks)\n",
    "# 2. Crop image and resize \n",
    "rect = preprocess.detectFace(image_aligned)\n",
    "croppedResizedImage, croppedResizedLandmarks = preprocess.cropResize(image_aligned, rect, 96, landmarks_aligned)\n",
    "# 3. Use grayscale\n",
    "finalImage = cv2.cvtColor(croppedResizedImage, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de900b2-995f-4b2f-9c45-1276eef23775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First\n",
    "image_ = image.copy()\n",
    "for (x, y) in landmarks:\n",
    "    cv2.circle(image_, (int(x), int(y)), 2, (0, 0, 255), -1)\n",
    "plt.imshow(image_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf54f0f-d006-4f33-a5ee-803c37dda4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second\n",
    "image_aligned_ = image_aligned.copy()\n",
    "for (x, y) in landmarks_aligned:\n",
    "    cv2.circle(image_aligned_, (int(x), int(y)), 2, (0, 0, 255), -1)\n",
    "plt.imshow(image_aligned_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea4d544-7432-466e-993f-7807c2db3147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third\n",
    "croppedResizedImage_ = croppedResizedImage.copy()\n",
    "for (x, y) in croppedResizedLandmarks:\n",
    "    cv2.circle(croppedResizedImage_, (int(x), int(y)), 1, (0, 0, 255), -1)\n",
    "plt.imshow(croppedResizedImage_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001efc3-082e-4847-b5a3-69d664199a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUlabels = preprocess.labelsetCK(AU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5ac2c4-3d01-4eb2-a6f5-a74a69eec657",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d4762f-e2c3-498b-aa65-58b1c68c44ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "participants.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b663ca84-17c2-46c8-b540-cfcac564a04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e11a094-60bd-4029-a920-25fb60d93b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = models.Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu',input_shape=(96, 96, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(96, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(39, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c921756c-5a1b-4a2c-b423-77ca1a85f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {'metric' :multi.fbeta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd9a1d-824c-41b3-84f7-e4359fd51485",
   "metadata": {},
   "outputs": [],
   "source": [
    "fbeta = multi.fbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba902832-1fce-4185-9579-f1019647cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(learning_rate=0.001, momentum=0.9)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n",
    "\n",
    "history = model.fit(train_images, train_labels, epochs=10, \n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b26b24-9814-4055-8e82-91b540ce12fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6c1d29-a613-4ae7-8136-8afd25069dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0c08cd-7285-4386-9b5d-c839c0e154b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.saving.saved_model import load as saved_model_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148e0e8a-61e9-48b2-b29d-6afa507ec02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = load_model('my_model.h5', custom_objects={'fbeta': multi.fbeta})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5a5336-1394-454d-a4c9-1470838f0288",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUNew, landmarksNew, imageNew, imageLandNew = preprocess.landmarks_AU_CK('S999', '001', folder)\n",
    "image_alignedNew, landmarks_alignedNew = preprocess.alignFromLandmarks(imageNew, landmarksNew)\n",
    "rectNew = preprocess.detectFace (image_alignedNew)\n",
    "croppedResizedImage = preprocess.cropResizeImage(image, rectNew, 96)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6252b7f9-fe3e-4fa1-ad80-9831f2df5bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(croppedResizedImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19632f31-dbce-4539-9576-dd1c472b0841",
   "metadata": {},
   "outputs": [],
   "source": [
    "croppedResizedImage.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4897743f-1be0-43e7-b491-99095fa3afb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8e4df1-fd67-4a67-816d-35bc74e2eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82f213b-2e94-4715-90c5-b907581a9ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred = model2.predict(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8146c31-c0ca-4c80-8738-1e25610f9c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = tf.expand_dims(train_images[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b721894-7b59-4c7d-a6be-c5ee296c678e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
